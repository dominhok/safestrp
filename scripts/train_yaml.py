#!/usr/bin/env python3
"""
SafeStrp Training Script with YAML Config

ê¹”ë”í•œ YAML ê¸°ë°˜ ì„¤ì • ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸.
í•µì‹¬ ì„¤ì •ë§Œ YAMLë¡œ ì»¨íŠ¸ë¡¤í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” í•©ë¦¬ì  ê¸°ë³¸ê°’ ì‚¬ìš©.
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# ë¦¬íŒ©í† ë§ëœ êµ¬ì¡°ì— ë§ëŠ” YAML config ì‹œìŠ¤í…œ
from configs.yaml_config import load_training_config, save_config, quick_config

# ë¦¬íŒ©í† ë§ëœ êµ¬ì¡°ì— ë§ëŠ” import
from src.core.model import ThreeTaskDSPNet
from src.losses.multitask import SimpleTwoTaskLoss
from src.data.loaders import create_dataset


class YAMLTrainer:
    """YAML config ê¸°ë°˜ ê¹”ë”í•œ í›ˆë ¨ í´ë˜ìŠ¤."""
    
    def __init__(self, config_path: str = None):
        """
        Initialize trainer with YAML config.
        
        Args:
            config_path: YAML config íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ íƒìƒ‰)
        """
        self.config = load_training_config(config_path)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._setup_device()
        
        # ëª¨ë¸ ìƒì„±
        self.model = self._create_model()
        
        # ì†ì‹¤í•¨ìˆ˜
        self.criterion = self._create_loss_function()
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # ë°ì´í„°ë¡œë”
        self.train_loader, self.val_loader = self._create_dataloaders()
        
        # Mixed precision scaler
        self.scaler = self._create_scaler()
        
        # ë¡œê¹…
        self.writer = self._create_tensorboard_writer()
        
        # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
        self.best_loss = float('inf')
        self.current_epoch = 0
        
        print("âœ… YAML Trainer ì´ˆê¸°í™” ì™„ë£Œ")
        self._print_config_summary()
    
    def _setup_device(self) -> torch.device:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •."""
        device_setting = self.config.system.device
        
        if device_setting == "auto":
            if torch.cuda.is_available():
                device = torch.device("cuda")
                print(f"ğŸš€ GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
            else:
                device = torch.device("cpu")
                print("ğŸ’» CPU ì‚¬ìš©")
        else:
            device = torch.device(device_setting)
            print(f"ğŸ¯ ì‚¬ìš©ì ì§€ì • ë””ë°”ì´ìŠ¤: {device}")
        
        return device
    
    def _create_model(self) -> ThreeTaskDSPNet:
        """ëª¨ë¸ ìƒì„±."""
        model = ThreeTaskDSPNet(
            num_detection_classes=self.config.model.num_detection_classes,
            num_surface_classes=self.config.model.num_surface_classes,
            input_size=tuple(self.config.model.input_size),
            pretrained_backbone=self.config.model.pretrained_backbone
        )
        return model.to(self.device)
    
    def _create_loss_function(self) -> SimpleTwoTaskLoss:
        """ì†ì‹¤í•¨ìˆ˜ ìƒì„±."""
        return SimpleTwoTaskLoss(
            detection_weight=self.config.training.loss_weights.detection,
            surface_weight=self.config.training.loss_weights.surface,
            distance_weight=self.config.training.loss_weights.depth
        )
    
    def _create_optimizer(self) -> optim.Optimizer:
        """ì˜µí‹°ë§ˆì´ì € ìƒì„±."""
        optimizer_name = self.config.training.optimizer.lower()
        
        if optimizer_name == "adamw":
            return optim.AdamW(
                self.model.parameters(),
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay
            )
        elif optimizer_name == "adam":
            return optim.Adam(
                self.model.parameters(),
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay
            )
        elif optimizer_name == "sgd":
            return optim.SGD(
                self.model.parameters(),
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay,
                momentum=0.9
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜µí‹°ë§ˆì´ì €: {optimizer_name}")
    
    def _create_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±."""
        scheduler_name = self.config.training.scheduler.lower()
        
        if scheduler_name == "reducelronplateau":
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=0.5,
                patience=self.config.training.patience,
                verbose=True
            )
        elif scheduler_name == "cosineannealingwarmrestarts":
            return optim.lr_scheduler.CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=10,
                T_mult=2
            )
        elif scheduler_name == "none":
            return None
        else:
            print(f"âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬: {scheduler_name}. ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None
    
    def _create_dataloaders(self):
        """ë°ì´í„°ë¡œë” ìƒì„±."""
        return create_dataset(
            base_dir=self.config.dataset.base_dir,
            batch_size=self.config.training.batch_size,
            num_workers=self.config.system.num_workers,
            max_samples=self.config.dataset.max_samples
        )
    
    def _create_scaler(self):
        """Mixed precision scaler ìƒì„±."""
        if self.config.training.mixed_precision and self.device.type == 'cuda':
            return torch.amp.GradScaler('cuda')
        return None
    
    def _create_tensorboard_writer(self):
        """TensorBoard writer ìƒì„±."""
        if self.config.logging.tensorboard:
            log_dir = os.path.join(self.config.logging.log_dir, "tensorboard")
            os.makedirs(log_dir, exist_ok=True)
            return SummaryWriter(log_dir)
        return None
    
    def _print_config_summary(self):
        """ì„¤ì • ìš”ì•½ ì¶œë ¥."""
        print("\n" + "="*50)
        print("ğŸ“‹ í›ˆë ¨ ì„¤ì • ìš”ì•½")
        print("="*50)
        print(f"ğŸ—ï¸  ëª¨ë¸: Detection({self.config.model.num_detection_classes}) + Surface({self.config.model.num_surface_classes}) + Depth")
        print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {self.config.training.batch_size}")
        print(f"ğŸ¯ í•™ìŠµë¥ : {self.config.training.learning_rate}")
        print(f"ğŸƒ ì—í¬í¬: {self.config.training.epochs}")
        print(f"âš–ï¸  ì†ì‹¤ ê°€ì¤‘ì¹˜: Det={self.config.training.loss_weights.detection}, Surf={self.config.training.loss_weights.surface}, Depth={self.config.training.loss_weights.depth}")
        print(f"ğŸ”§ ì˜µí‹°ë§ˆì´ì €: {self.config.training.optimizer}")
        print(f"ğŸ“ˆ ìŠ¤ì¼€ì¤„ëŸ¬: {self.config.training.scheduler}")
        print(f"âš¡ Mixed Precision: {self.config.training.mixed_precision}")
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: {self.config.checkpoint.save_dir}")
        print("="*50 + "\n")
    
    def _prepare_targets(self, batch):
        """ë°°ì¹˜ì—ì„œ íƒ€ê²Ÿ ì¤€ë¹„."""
        targets = {}
        
        # Surface segmentation targets
        if 'surface_masks' in batch:
            targets['surface_masks'] = batch['surface_masks'].to(
                self.device, non_blocking=self.config.system.non_blocking
            )
        
        # Detection targets (SSD í˜•íƒœ)
        if 'detection_boxes' in batch:
            detection_boxes_list = []
            for boxes in batch['detection_boxes']:
                if torch.is_tensor(boxes) and boxes.size(-1) >= 4:
                    bbox_only = boxes[:, :4]  # bbox ì¢Œí‘œë§Œ ì¶”ì¶œ
                    detection_boxes_list.append(bbox_only.to(
                        self.device, non_blocking=self.config.system.non_blocking
                    ))
                else:
                    detection_boxes_list.append(torch.zeros(0, 4, dtype=torch.float32).to(self.device))
            
            targets['detection_boxes'] = detection_boxes_list
        
        # Detection labels
        if 'detection_labels' in batch:
            detection_labels_list = []
            for labels in batch['detection_labels']:
                if torch.is_tensor(labels):
                    detection_labels_list.append(labels.to(
                        self.device, non_blocking=self.config.system.non_blocking
                    ))
                else:
                    detection_labels_list.append(torch.zeros(0, dtype=torch.long).to(self.device))
            
            targets['detection_labels'] = detection_labels_list
        
        # Depth targets
        if 'depth_maps' in batch:
            targets['depth_maps'] = batch['depth_maps'].to(
                self.device, non_blocking=self.config.system.non_blocking
            )
            
        if 'confidence_masks' in batch:
            targets['confidence_masks'] = batch['confidence_masks'].to(
                self.device, non_blocking=self.config.system.non_blocking
            )
        
        return targets
    
    def train_epoch(self, epoch: int) -> dict:
        """í•œ ì—í¬í¬ í›ˆë ¨."""
        self.model.train()
        total_loss = 0.0
        task_losses = {'detection': 0.0, 'surface': 0.0, 'depth': 0.0}
        task_counts = {'detection': 0, 'surface': 0, 'depth': 0}
        
        pbar = tqdm(self.train_loader, desc=f"ğŸ¯ Epoch {epoch+1}/{self.config.training.epochs}")
        
        for batch_idx, batch in enumerate(pbar):
            # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            images = batch['images'].to(self.device, non_blocking=self.config.system.non_blocking)
            
            # íƒ€ê²Ÿ ì¤€ë¹„
            targets = self._prepare_targets(batch)
            
            # Forward pass
            if self.scaler:
                with torch.amp.autocast('cuda'):
                    outputs = self.model(images)
                    loss, loss_details = self.criterion(outputs, targets)
            else:
                outputs = self.model(images)
                loss, loss_details = self.criterion(outputs, targets)
            
            # Backward pass
            self.optimizer.zero_grad()
            
            if self.scaler:
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.training.max_grad_norm)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.training.max_grad_norm)
                self.optimizer.step()
            
            # ì†ì‹¤ ëˆ„ì 
            total_loss += loss.item()
            for task, task_loss in loss_details.items():
                if task in task_losses:
                    task_losses[task] += task_loss
                    task_counts[task] += 1
            
            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            if batch_idx % self.config.logging.print_interval == 0:
                avg_loss = total_loss / (batch_idx + 1)
                pbar.set_postfix({
                    'Loss': f'{avg_loss:.4f}',
                    'Det': f'{task_losses["detection"]/(task_counts["detection"]+1e-6):.4f}',
                    'Surf': f'{task_losses["surface"]/(task_counts["surface"]+1e-6):.4f}',
                    'Depth': f'{task_losses["depth"]/(task_counts["depth"]+1e-6):.4f}'
                })
        
        # í‰ê·  ì†ì‹¤ ê³„ì‚°
        avg_loss = total_loss / len(self.train_loader)
        avg_task_losses = {task: loss / max(count, 1) for task, loss, count in 
                          zip(task_losses.keys(), task_losses.values(), task_counts.values())}
        
        return {
            'total_loss': avg_loss,
            'task_losses': avg_task_losses
        }
    
    def validate(self) -> dict:
        """ê²€ì¦."""
        self.model.eval()
        total_loss = 0.0
        task_losses = {'detection': 0.0, 'surface': 0.0, 'depth': 0.0}
        task_counts = {'detection': 0, 'surface': 0, 'depth': 0}
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="ğŸ” Validation"):
                images = batch['images'].to(self.device, non_blocking=self.config.system.non_blocking)
                targets = self._prepare_targets(batch)
                
                outputs = self.model(images)
                loss, loss_details = self.criterion(outputs, targets)
                
                total_loss += loss.item()
                for task, task_loss in loss_details.items():
                    if task in task_losses:
                        task_losses[task] += task_loss
                        task_counts[task] += 1
        
        avg_loss = total_loss / len(self.val_loader)
        avg_task_losses = {task: loss / max(count, 1) for task, loss, count in 
                          zip(task_losses.keys(), task_losses.values(), task_counts.values())}
        
        return {
            'total_loss': avg_loss,
            'task_losses': avg_task_losses
        }
    
    def save_checkpoint(self, epoch: int, loss: float, is_best: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥."""
        os.makedirs(self.config.checkpoint.save_dir, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'loss': loss,
            'config': self.config.to_dict()
        }
        
        # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if epoch % self.config.checkpoint.save_interval == 0:
            checkpoint_path = os.path.join(self.config.checkpoint.save_dir, f'checkpoint_epoch_{epoch}.pth')
            torch.save(checkpoint, checkpoint_path)
            print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
        
        # ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if is_best and self.config.checkpoint.keep_best:
            best_path = os.path.join(self.config.checkpoint.save_dir, 'best_model.pth')
            torch.save(checkpoint, best_path)
            print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_path}")
    
    def train(self):
        """ì „ì²´ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤."""
        print("ğŸš€ í›ˆë ¨ ì‹œì‘!")
        
        for epoch in range(self.config.training.epochs):
            self.current_epoch = epoch
            
            # í›ˆë ¨
            train_results = self.train_epoch(epoch)
            
            # ê²€ì¦
            val_results = self.validate()
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            if self.scheduler:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_results['total_loss'])
                else:
                    self.scheduler.step()
            
            # ë¡œê¹…
            current_lr = self.optimizer.param_groups[0]['lr']
            print(f"\nğŸ“Š Epoch {epoch+1}/{self.config.training.epochs}")
            print(f"   í›ˆë ¨ ì†ì‹¤: {train_results['total_loss']:.4f}")
            print(f"   ê²€ì¦ ì†ì‹¤: {val_results['total_loss']:.4f}")
            print(f"   í•™ìŠµë¥ : {current_lr:.2e}")
            
            if self.writer:
                self.writer.add_scalar('Loss/Train', train_results['total_loss'], epoch)
                self.writer.add_scalar('Loss/Val', val_results['total_loss'], epoch)
                self.writer.add_scalar('Learning_Rate', current_lr, epoch)
                
                for task, loss in train_results['task_losses'].items():
                    self.writer.add_scalar(f'Loss_Train/{task}', loss, epoch)
                for task, loss in val_results['task_losses'].items():
                    self.writer.add_scalar(f'Loss_Val/{task}', loss, epoch)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            is_best = val_results['total_loss'] < self.best_loss
            if is_best:
                self.best_loss = val_results['total_loss']
            
            self.save_checkpoint(epoch, val_results['total_loss'], is_best)
        
        print("ğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
        if self.writer:
            self.writer.close()


def main():
    """ë©”ì¸ í•¨ìˆ˜."""
    parser = argparse.ArgumentParser(description='SafeStrp YAML Training')
    parser.add_argument('--config', type=str, default=None, 
                       help='YAML config íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: ìë™ íƒìƒ‰)')
    parser.add_argument('--quick', action='store_true',
                       help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì¶•ì†Œ ì„¤ì • ì‚¬ìš©')
    
    args = parser.parse_args()
    
    try:
        if args.quick:
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© ì„¤ì •
            config = quick_config(
                batch_size=4,
                epochs=5,
                max_samples=100
            )
            print("ğŸƒ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
        else:
            config = None  # ìë™ ë¡œë“œ
        
        trainer = YAMLTrainer(args.config)
        trainer.train()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main() 