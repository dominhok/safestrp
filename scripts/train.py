#!/usr/bin/env python3
"""
TwoTaskDSPNet Training Script

Detection + Surface 2íƒœìŠ¤í¬ ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµì„ ìœ„í•œ ë©”ì¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸.
ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import time
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ëŠ” import
from src.model import TwoTaskDSPNet
from src.losses import SimpleTwoTaskLoss, AdvancedTwoTaskLoss
from utils.dataset import create_massive_dataset
from configs.config import Config, get_quick_test_config, get_full_training_config, get_massive_dataset_config


class TwoTaskTrainer:
    """
    2íƒœìŠ¤í¬ ë©€í‹°íƒœìŠ¤í¬ íŠ¸ë ˆì´ë„ˆ.
    
    ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ êµ¬ì¡°ì™€ ì„¤ì • ì‹œìŠ¤í…œì„ í™œìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: Config):
        """
        Initialize trainer with configuration.
        
        Args:
            config: Configuration object with all parameters
        """
        self.config = config
        
        # Device ì„¤ì •
        self.device = self._setup_device()
        
        # ëª¨ë¸ ìƒì„±
        self.model = self._create_model()
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        self.train_loader, self.val_loader = self._create_dataloaders()
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # ì†ì‹¤í•¨ìˆ˜ - ì•ˆì •ì„±ì„ ìœ„í•´ Simple ë²„ì „ ì‚¬ìš©
        print("ğŸ“ Simple Loss ì‚¬ìš© (ì•ˆì •í™” ë²„ì „)")
        if hasattr(self.config, 'loss'):
            # massive_dataset_config ì‚¬ìš©ì‹œ
            detection_weight = getattr(self.config.loss, 'detection_weight', 1.0)
            surface_weight = getattr(self.config.loss, 'surface_weight', 1.0)
            distance_weight = getattr(self.config.loss, 'distance_weight', 0.5)
        else:
            # ê¸°ë³¸ Config í´ë˜ìŠ¤ ì‚¬ìš©ì‹œ
            detection_weight = self.config.training.detection_weight
            surface_weight = self.config.training.surface_weight
            distance_weight = self.config.training.distance_weight
            
        self.criterion = SimpleTwoTaskLoss(
            detection_weight=detection_weight,
            surface_weight=surface_weight,
            distance_weight=distance_weight
        )
        
        # ë””ë ‰í† ë¦¬ ìƒì„± - config êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ‘ê·¼
        if hasattr(self.config, 'save'):
            checkpoint_dir = self.config.save.checkpoint_dir
            log_dir = getattr(self.config.logging, 'log_dir', 'logs/two_task_training')
        else:
            checkpoint_dir = self.config.data.checkpoint_dir
            log_dir = self.config.data.log_dir
            
        os.makedirs(checkpoint_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
        
        # í…ì„œë³´ë“œ
        use_tensorboard = False
        if hasattr(self.config, 'logging'):
            use_tensorboard = getattr(self.config.logging, 'use_tensorboard', True)
        elif hasattr(self.config, 'system'):
            use_tensorboard = self.config.system.tensorboard_log
            
        self.writer = SummaryWriter(log_dir=log_dir) if use_tensorboard else None
        
        # í›ˆë ¨ ìƒíƒœ
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        
        # Mixed precision ì„¤ì •
        if hasattr(self.config, 'system') and self.config.system.mixed_precision and self.device == "cuda":
            print("âš¡ Mixed Precision í›ˆë ¨ í™œì„±í™”")
            self.scaler = torch.amp.GradScaler('cuda')
        elif hasattr(self.config, 'training') and self.config.training.use_amp and self.device == "cuda":
            print("âš¡ Mixed Precision í›ˆë ¨ í™œì„±í™”")
            self.scaler = torch.amp.GradScaler('cuda')
        else:
            self.scaler = None
        
        print(f"âœ… TwoTaskTrainer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in self.model.parameters()):,}ê°œ")
        print(f"   í›ˆë ¨ ìƒ˜í”Œ: {len(self.train_loader.dataset)}ê°œ")
        print(f"   ê²€ì¦ ìƒ˜í”Œ: {len(self.val_loader.dataset)}ê°œ")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _setup_device(self) -> str:
        """Setup device based on configuration."""
        if self.config.system.device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            device = self.config.system.device
        
        if device == "cuda":
            print(f"ğŸš€ GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
            print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        
        return device
    
    def _create_model(self) -> TwoTaskDSPNet:
        """Create model based on configuration."""
        # config êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ‘ê·¼
        if hasattr(self.config, 'model') and hasattr(self.config.model, 'num_detection_classes'):
            # ê¸°ë³¸ Config í´ë˜ìŠ¤ ì‚¬ìš©ì‹œ
            num_detection_classes = self.config.model.num_detection_classes
            num_surface_classes = self.config.model.num_surface_classes
            input_size = self.config.model.input_size
            pretrained_backbone = self.config.model.pretrained_backbone
        else:
            # get_massive_dataset_configì˜ SimpleNamespace ì‚¬ìš©ì‹œ
            num_detection_classes = getattr(self.config.model, 'num_classes', 27)
            num_surface_classes = getattr(self.config.model, 'surface_classes', 4)  
            input_size = getattr(self.config.model, 'input_size', (512, 512))
            pretrained_backbone = getattr(self.config.model, 'pretrained', True)
        
        model = TwoTaskDSPNet(
            num_detection_classes=num_detection_classes,
            num_surface_classes=num_surface_classes,
            input_size=input_size,
            pretrained_backbone=pretrained_backbone
        )
        
        model = model.to(self.device)
        
        return model
    
    def _create_dataloaders(self):
        """Create data loaders based on configuration."""
        # ìƒˆë¡œìš´ config êµ¬ì¡° ì²´í¬
        if hasattr(self.config, 'dataset'):
            # ìƒˆë¡œìš´ massive_dataset_config ì‚¬ìš©
            return create_massive_dataset(
                base_dir=self.config.dataset.base_dir,
                batch_size=self.config.dataset.batch_size,
                num_workers=self.config.dataset.num_workers,
                max_samples=self.config.dataset.max_samples,
                use_depth=self.config.dataset.use_depth
            )
        else:
            # ê¸°ì¡´ Config í´ë˜ìŠ¤ ì‚¬ìš©
            return create_massive_dataset(
                batch_size=self.config.training.batch_size,
                num_workers=self.config.training.num_workers,
                max_samples=self.config.training.max_samples,
                base_dir=self.config.data.base_dir
            )
    
    def _create_optimizer(self):
        """Create optimizer based on configuration."""
        # config êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ‘ê·¼
        if hasattr(self.config, 'optimizer'):
            # massive_dataset_config ì‚¬ìš©ì‹œ
            optimizer_type = getattr(self.config.optimizer, 'type', 'adamw').lower()
            learning_rate = getattr(self.config.optimizer, 'lr', 1e-4)
            weight_decay = getattr(self.config.optimizer, 'weight_decay', 1e-4)
        else:
            # ê¸°ë³¸ Config í´ë˜ìŠ¤ ì‚¬ìš©ì‹œ
            optimizer_type = self.config.training.optimizer_type.lower()
            learning_rate = self.config.training.learning_rate
            weight_decay = self.config.training.weight_decay
            
        if optimizer_type == "adam":
            return optim.Adam(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        elif optimizer_type == "adamw":
            return optim.AdamW(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        elif optimizer_type == "sgd":
            return optim.SGD(
                self.model.parameters(),
                lr=learning_rate,
                momentum=0.9,
                weight_decay=weight_decay
            )
        else:
            raise ValueError(f"Unsupported optimizer: {optimizer_type}")
    
    def _create_scheduler(self):
        """Create learning rate scheduler based on configuration."""
        # config êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ‘ê·¼
        if hasattr(self.config, 'scheduler'):
            # massive_dataset_config ì‚¬ìš©ì‹œ
            scheduler_type = getattr(self.config.scheduler, 'type', 'cosine').lower()
            epochs = getattr(self.config.training, 'epochs', 50)
        else:
            # ê¸°ë³¸ Config í´ë˜ìŠ¤ ì‚¬ìš©ì‹œ
            scheduler_type = self.config.training.scheduler_type.lower()
            epochs = self.config.training.epochs
            
        if scheduler_type == "cosine":
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=epochs,
                eta_min=1e-6
            )
        elif scheduler_type == "step":
            step_size = getattr(self.config.training, 'step_size', 15)
            gamma = getattr(self.config.training, 'gamma', 0.1)
            return optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=step_size,
                gamma=gamma
            )
        elif scheduler_type in ["plateau", "reduceonplateau"]:
            factor = getattr(self.config.scheduler, 'factor', 0.5) if hasattr(self.config, 'scheduler') else 0.1
            patience = getattr(self.config.scheduler, 'patience', 5) if hasattr(self.config, 'scheduler') else self.config.training.patience // 2
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                patience=patience,
                factor=factor
            )
        else:
            return None
    
    def train_epoch(self, epoch: int) -> dict:
        """Train one epoch."""
        self.model.train()
        total_loss = 0.0
        task_losses = {'detection': 0.0, 'surface': 0.0}
        task_counts = {'detection': 0, 'surface': 0}
        
        pbar = tqdm(self.train_loader, desc=f"ğŸ¯ Epoch {epoch+1}/{self.config.training.epochs}")
        
        for batch_idx, batch in enumerate(pbar):
            # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            images = batch['images'].to(self.device, non_blocking=self.config.system.non_blocking)
            
            # íƒ€ê²Ÿ ì¤€ë¹„
            targets = self._prepare_targets(batch)
            
            # Mixed precision ì‚¬ìš©ì‹œ
            if self.scaler:
                with torch.amp.autocast('cuda'):
                    outputs = self.model(images)
                    loss, loss_details = self.criterion(outputs, targets)
            else:
                outputs = self.model(images)
                loss, loss_details = self.criterion(outputs, targets)
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            
            if self.scaler:
                # Scaled backward for mixed precision
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                
                # Gradient clipping - config êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ‘ê·¼
                max_grad_norm = getattr(self.config.training, 'max_grad_norm', 1.0)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                
                # Gradient clipping - config êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ‘ê·¼  
                max_grad_norm = getattr(self.config.training, 'max_grad_norm', 1.0)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)
                
                self.optimizer.step()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            total_loss += loss.item()
            
            # íƒœìŠ¤í¬ë³„ ì†ì‹¤ ì—…ë°ì´íŠ¸
            tasks = batch['tasks']
            for task in tasks:
                if task == 'detection':
                    task_counts['detection'] += 1
                    if 'cls_loss' in loss_details:
                        det_loss = loss_details['cls_loss']
                        if torch.is_tensor(det_loss):
                            det_loss = det_loss.item()
                        task_losses['detection'] += det_loss
                elif task == 'surface':
                    task_counts['surface'] += 1
                    if 'surface_loss' in loss_details:
                        surf_loss = loss_details['surface_loss']
                        if torch.is_tensor(surf_loss):
                            surf_loss = surf_loss.item()
                        task_losses['surface'] += surf_loss
            
            # ì§„í–‰ë¥  í‘œì‹œ ì—…ë°ì´íŠ¸
            pbar.set_postfix({
                'Loss': f"{loss.item():.4f}",
                'Det': f"{task_losses['detection']/(task_counts['detection']+1e-8):.4f}",
                'Surf': f"{task_losses['surface']/(task_counts['surface']+1e-8):.4f}",
                'LR': f"{self.optimizer.param_groups[0]['lr']:.2e}"
            })
            
            # í…ì„œë³´ë“œ ë¡œê¹…
            if self.writer and batch_idx % self.config.system.print_frequency == 0:
                global_step = epoch * len(self.train_loader) + batch_idx
                self.writer.add_scalar('Train/BatchLoss', loss.item(), global_step)
                self.writer.add_scalar('Train/LearningRate', self.optimizer.param_groups[0]['lr'], global_step)
        
        # ì—í­ í†µê³„ ê³„ì‚°
        avg_loss = total_loss / len(self.train_loader)
        
        return {
            'avg_loss': avg_loss,
            'detection_loss': task_losses['detection'] / (task_counts['detection'] + 1e-8),
            'surface_loss': task_losses['surface'] / (task_counts['surface'] + 1e-8)
        }
    
    def _prepare_targets(self, batch: dict) -> dict:
        """Prepare targets for loss computation."""
        targets = {
            'has_detection': batch['has_detection'],
            'has_surface': batch['has_surface']
        }
        
        # **Detection targets** (ìˆ˜ì •ë¨ - ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ì²˜ë¦¬)
        if 'detection_boxes' in batch:
            # detection_boxesëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë“¤ì–´ì˜´ (ê°€ë³€ ê¸¸ì´ ë•Œë¬¸ì—)
            detection_boxes_list = batch['detection_boxes']
            
            # ê° ë°°ì¹˜ ì•„ì´í…œì˜ boxesë¥¼ deviceë¡œ ì´ë™í•˜ê³  4ì°¨ì› bboxë§Œ ì¶”ì¶œ
            processed_boxes = []
            for boxes in detection_boxes_list:
                if torch.is_tensor(boxes):
                    # 6ì°¨ì›ì—ì„œ 4ì°¨ì› bboxë§Œ ì¶”ì¶œ: [x1, y1, x2, y2, class_id, distance] -> [x1, y1, x2, y2]
                    if boxes.size(-1) >= 4:
                        bbox_only = boxes[:, :4]  # ì²« 4ê°œ ì°¨ì›ë§Œ ì‚¬ìš©
                        processed_boxes.append(bbox_only.to(
                            self.device, non_blocking=self.config.system.non_blocking
                        ))
                    else:
                        # ë¹ˆ í…ì„œ ìƒì„±
                        processed_boxes.append(torch.zeros(0, 4, dtype=torch.float32).to(self.device))
                else:
                    # boxesê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í…ì„œë¡œ ë³€í™˜í•˜ê³  4ì°¨ì›ë§Œ ì¶”ì¶œ
                    if len(boxes) > 0 and len(boxes[0]) >= 4:
                        bbox_only = [[box[0], box[1], box[2], box[3]] for box in boxes]
                        processed_boxes.append(torch.tensor(
                            bbox_only, dtype=torch.float32
                        ).to(self.device, non_blocking=self.config.system.non_blocking))
                    else:
                        processed_boxes.append(torch.zeros(0, 4, dtype=torch.float32).to(self.device))
            
            targets['detection_boxes'] = processed_boxes
        
        # Detection labelsëŠ” boxesì—ì„œ ì¶”ì¶œ (í´ë˜ìŠ¤ ì •ë³´ê°€ í¬í•¨ë¨)
        if 'detection_boxes' in batch:
            detection_labels_list = []
            for boxes in batch['detection_boxes']:
                if torch.is_tensor(boxes) and boxes.size(-1) >= 6:
                    # boxes í˜•íƒœ: [x1, y1, x2, y2, class_id, distance]
                    labels = boxes[:, 4].long()  # í´ë˜ìŠ¤ ID ì¶”ì¶œ
                    detection_labels_list.append(labels.to(
                        self.device, non_blocking=self.config.system.non_blocking
                    ))
                else:
                    # ë¹ˆ í…ì„œ ìƒì„±
                    detection_labels_list.append(torch.zeros(0, dtype=torch.long).to(self.device))
            
            targets['detection_labels'] = detection_labels_list
        
        # Distance targetsë„ boxesì—ì„œ ì¶”ì¶œ
        if 'detection_boxes' in batch:
            detection_distances_list = []
            for boxes in batch['detection_boxes']:
                if torch.is_tensor(boxes) and boxes.size(-1) >= 6:
                    # boxes í˜•íƒœ: [x1, y1, x2, y2, class_id, distance]
                    distances = boxes[:, 5]  # distance ì¶”ì¶œ
                    detection_distances_list.append(distances.to(
                        self.device, non_blocking=self.config.system.non_blocking
                    ))
                else:
                    # ë¹ˆ í…ì„œ ìƒì„±
                    detection_distances_list.append(torch.zeros(0, dtype=torch.float32).to(self.device))
            
            targets['detection_distances'] = detection_distances_list
        
        # Surface targets
        if 'surface_masks' in batch:
            targets['surface_masks'] = batch['surface_masks'].to(
                self.device, non_blocking=self.config.system.non_blocking
            )
        
        return targets
    
    def validate_epoch(self, epoch: int) -> dict:
        """Validate one epoch."""
        self.model.eval()
        total_loss = 0.0
        
        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc="ğŸ” Validation")
            
            for batch in pbar:
                images = batch['images'].to(self.device, non_blocking=self.config.system.non_blocking)
                targets = self._prepare_targets(batch)
                
                if self.scaler:
                    with torch.amp.autocast('cuda'):
                        outputs = self.model(images)
                        loss, _ = self.criterion(outputs, targets)
                else:
                    outputs = self.model(images)
                    loss, _ = self.criterion(outputs, targets)
                
                total_loss += loss.item()
                pbar.set_postfix({'Val Loss': f"{loss.item():.4f}"})
        
        avg_val_loss = total_loss / len(self.val_loader)
        return {'avg_loss': avg_val_loss}
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """Save model checkpoint."""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': self.config.to_dict(),
            'best_val_loss': self.best_val_loss
        }
        
        # Save latest checkpoint
        if self.config.system.save_last:
            latest_path = os.path.join(self.config.data.checkpoint_dir, 'latest_two_task.pth')
            torch.save(checkpoint, latest_path)
        
        # Save best checkpoint
        if is_best and self.config.system.save_best_only:
            best_path = os.path.join(self.config.data.checkpoint_dir, 'best_two_task.pth')
            torch.save(checkpoint, best_path)
            print(f"ğŸ’¾ Best model saved: {best_path}")
    
    def train(self):
        """Main training loop."""
        print(f"\nğŸš€ í›ˆë ¨ ì‹œì‘!")
        print("=" * 70)
        
        start_time = time.time()
        
        for epoch in range(self.config.training.epochs):
            # í›ˆë ¨
            train_metrics = self.train_epoch(epoch)
            
            # ê²€ì¦
            if epoch % self.config.training.val_frequency == 0:
                val_metrics = self.validate_epoch(epoch)
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì²´í¬
                is_best = val_metrics['avg_loss'] < self.best_val_loss
                if is_best:
                    self.best_val_loss = val_metrics['avg_loss']
                    self.patience_counter = 0
                else:
                    self.patience_counter += 1
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if epoch % self.config.training.save_frequency == 0 or is_best:
                    self.save_checkpoint(epoch, is_best)
                
                # í…ì„œë³´ë“œ ë¡œê¹…
                if self.writer:
                    self.writer.add_scalar('Train/EpochLoss', train_metrics['avg_loss'], epoch)
                    self.writer.add_scalar('Val/EpochLoss', val_metrics['avg_loss'], epoch)
                
                # ê²°ê³¼ ì¶œë ¥
                print(f"\nğŸ“Š Epoch {epoch+1} ê²°ê³¼:")
                print(f"   í›ˆë ¨ ì†ì‹¤: {train_metrics['avg_loss']:.4f}")
                print(f"   ê²€ì¦ ì†ì‹¤: {val_metrics['avg_loss']:.4f}")
                print(f"   ìµœê³  ì„±ëŠ¥: {self.best_val_loss:.4f}")
                print(f"   í•™ìŠµë¥ : {self.optimizer.param_groups[0]['lr']:.2e}")
                
                # Early stopping ì²´í¬
                if self.patience_counter >= self.config.training.patience:
                    print(f"â¹ï¸  Early stopping! (patience: {self.config.training.patience})")
                    break
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            if self.scheduler:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_metrics['avg_loss'])
                else:
                    self.scheduler.step()
        
        # í›ˆë ¨ ì™„ë£Œ
        total_time = time.time() - start_time
        print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ì´ ì‹œê°„: {total_time/3600:.2f}ì‹œê°„")
        print(f"   ìµœê³  ê²€ì¦ ì†ì‹¤: {self.best_val_loss:.4f}")
        
        if self.writer:
            self.writer.close()


def main():
    """Main function."""
    print("ğŸ¯ TwoTaskDSPNet í›ˆë ¨ ì‹œì‘")
    print("=" * 70)
    
    # ì„¤ì • ë¡œë“œ - ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì„¤ì • ì‚¬ìš©
    config = get_massive_dataset_config()  # ì „ì²´ ë°ì´í„°ì…‹ ì„¤ì •
    # config = get_full_training_config()  # ì „ì²´ í›ˆë ¨ ì„¤ì •
    # config = get_quick_test_config()  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì„¤ì •
    
    # ì„¤ì • ì¶œë ¥
    config.print_config()
    
    # ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
    print(f"\nğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:")
    print(f"   ê¸°ë³¸ ê²½ë¡œ: {config.data.base_dir}")
    print(f"   ìµœëŒ€ ìƒ˜í”Œ ìˆ˜: {config.dataset.max_samples:,}ê°œ")
    print(f"   ë°°ì¹˜ í¬ê¸°: {config.dataset.batch_size}")
    print(f"   Depth ì‚¬ìš©: {config.dataset.use_depth}")
    
    # ì¬í˜„ì„± ì„¤ì •
    if config.system.seed:
        torch.manual_seed(config.system.seed)
        np.random.seed(config.system.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(config.system.seed)
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨ ì‹œì‘
    trainer = TwoTaskTrainer(config)
    trainer.train()


if __name__ == "__main__":
    main() 