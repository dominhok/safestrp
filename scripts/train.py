#!/usr/bin/env python3
"""
TwoTaskDSPNet Training Script

Detection + Surface 2íƒœìŠ¤í¬ ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµì„ ìœ„í•œ ë©”ì¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸.
ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤.
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import time
from tqdm import tqdm
from typing import Dict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ëŠ” import
from src.core.model import ThreeTaskDSPNet
from src.losses.multitask import UberNetMTPSLLoss
from src.data.loaders import create_massive_dataset, create_ubernet_mtpsl_dataset
from configs.config import Config, get_quick_test_config, get_full_training_config, get_massive_dataset_config, get_subset_training_config


class TwoTaskTrainer:
    """
    2íƒœìŠ¤í¬ ë©€í‹°íƒœìŠ¤í¬ íŠ¸ë ˆì´ë„ˆ.
    
    ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ êµ¬ì¡°ì™€ ì„¤ì • ì‹œìŠ¤í…œì„ í™œìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, config: Config):
        """
        Initialize trainer with configuration.
        
        Args:
            config: Configuration object with all parameters
        """
        self.config = config
        
        # Device ì„¤ì •
        self.device = self._setup_device()
        
        # ëª¨ë¸ ìƒì„±
        self.model = self._create_model()
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        self.train_loader, self.val_loader = self._create_dataloaders()
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # ì†ì‹¤í•¨ìˆ˜ - UberNet + MTPSL ë°©ì‹ ì‚¬ìš© (ìˆ˜ì •ë¨)
        print("ğŸ“ UberNet + MTPSL Loss ì‚¬ìš© (partial label + cross-task consistency)")
        if hasattr(self.config, 'loss'):
            # massive_dataset_config ì‚¬ìš©ì‹œ
            detection_weight = getattr(self.config.loss, 'detection_weight', 1.0)
            surface_weight = getattr(self.config.loss, 'surface_weight', 1.0)
            depth_weight = getattr(self.config.loss, 'depth_weight', 0.5)
        else:
            # ê¸°ë³¸ Config í´ë˜ìŠ¤ ì‚¬ìš©ì‹œ
            detection_weight = self.config.training.detection_weight
            surface_weight = self.config.training.surface_weight
            depth_weight = self.config.training.depth_weight
            
        # UberNet + MTPSL ì†ì‹¤í•¨ìˆ˜ ì‚¬ìš© (partial label handling + cross-task consistency)
        from src.losses.multitask import UberNetMTPSLLoss
        self.criterion = UberNetMTPSLLoss(
            detection_weight=detection_weight,
            surface_weight=surface_weight,
            depth_weight=depth_weight,
            cross_task_weight=0.1,  # Cross-task consistency ê°€ì¤‘ì¹˜
            regularization_weight=0.1,
            num_classes=num_detection_classes if 'num_detection_classes' in locals() else 29
        )
        
        # ë””ë ‰í† ë¦¬ ìƒì„± - config êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ‘ê·¼
        if hasattr(self.config, 'save'):
            checkpoint_dir = self.config.save.checkpoint_dir
            log_dir = getattr(self.config.logging, 'log_dir', 'logs/two_task_training')
        else:
            checkpoint_dir = self.config.data.checkpoint_dir
            log_dir = self.config.data.log_dir
            
        os.makedirs(checkpoint_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
        
        # í…ì„œë³´ë“œ
        use_tensorboard = False
        if hasattr(self.config, 'logging'):
            use_tensorboard = getattr(self.config.logging, 'use_tensorboard', True)
        elif hasattr(self.config, 'system'):
            use_tensorboard = self.config.system.tensorboard_log
            
        self.writer = SummaryWriter(log_dir=log_dir) if use_tensorboard else None
        
        # í›ˆë ¨ ìƒíƒœ
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        
        # Mixed precision ì„¤ì •
        if hasattr(self.config, 'system') and self.config.system.mixed_precision and self.device == "cuda":
            print("âš¡ Mixed Precision í›ˆë ¨ í™œì„±í™”")
            self.scaler = torch.amp.GradScaler('cuda')
        elif hasattr(self.config, 'training') and self.config.training.use_amp and self.device == "cuda":
            print("âš¡ Mixed Precision í›ˆë ¨ í™œì„±í™”")
            self.scaler = torch.amp.GradScaler('cuda')
        else:
            self.scaler = None
        
        print(f"âœ… TwoTaskTrainer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in self.model.parameters()):,}ê°œ")
        print(f"   í›ˆë ¨ ìƒ˜í”Œ: {len(self.train_loader.dataset)}ê°œ")
        print(f"   ê²€ì¦ ìƒ˜í”Œ: {len(self.val_loader.dataset)}ê°œ")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _setup_device(self) -> str:
        """Setup device based on configuration."""
        if self.config.system.device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            device = self.config.system.device
        
        if device == "cuda":
            print(f"ğŸš€ GPU ì‚¬ìš©: {torch.cuda.get_device_name()}")
            print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        
        return device
    
    def _create_model(self) -> ThreeTaskDSPNet:
        """Create and initialize the model."""
        print(f"ğŸ“¦ ëª¨ë¸ ìƒì„± ì¤‘...")
        
        # config êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ‘ê·¼
        if hasattr(self.config, 'model') and hasattr(self.config.model, 'num_detection_classes'):
            # ê¸°ë³¸ Config í´ë˜ìŠ¤ ì‚¬ìš©ì‹œ
            num_detection_classes = self.config.model.num_detection_classes
            num_surface_classes = self.config.model.num_surface_classes
            input_size = self.config.model.input_size
            pretrained_backbone = self.config.model.pretrained_backbone
        else:
            # get_massive_dataset_configì˜ SimpleNamespace ì‚¬ìš©ì‹œ
            num_detection_classes = getattr(self.config.model, 'num_classes', 29)
            num_surface_classes = getattr(self.config.model, 'surface_classes', 7)  
            input_size = getattr(self.config.model, 'input_size', (512, 512))
            pretrained_backbone = getattr(self.config.model, 'pretrained', True)
        
        model = ThreeTaskDSPNet(
            num_detection_classes=num_detection_classes,
            num_surface_classes=num_surface_classes,
            input_size=input_size,
            pretrained_backbone=pretrained_backbone
        )
        
        model = model.to(self.device)
        
        return model
    
    def _create_dataloaders(self):
        """Create data loaders based on configuration."""
        # UberNet + MTPSL ìŠ¤íƒ€ì¼ ë°ì´í„°ì…‹ ì‚¬ìš© (ìˆ˜ì •ë¨)
        if hasattr(self.config, 'dataset'):
            # ìƒˆë¡œìš´ massive_dataset_config ì‚¬ìš©ì‹œ - UberNet + MTPSL ë°©ì‹
            return create_ubernet_mtpsl_dataset(
                base_dir=self.config.dataset.base_dir,
                batch_size=self.config.dataset.batch_size,
                num_workers=self.config.dataset.num_workers,
                max_samples=self.config.dataset.max_samples
            )
        else:
            # ê¸°ë³¸ Config í´ë˜ìŠ¤ ì‚¬ìš©ì‹œ - UberNet + MTPSL ë°©ì‹
            return create_ubernet_mtpsl_dataset(
                batch_size=self.config.training.batch_size,
                num_workers=self.config.training.num_workers,
                max_samples=self.config.training.max_samples,
                base_dir=self.config.data.base_dir
            )
    
    def _create_optimizer(self):
        """Create optimizer based on configuration."""
        # config êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ‘ê·¼
        if hasattr(self.config, 'optimizer'):
            # massive_dataset_config ì‚¬ìš©ì‹œ
            optimizer_type = getattr(self.config.optimizer, 'type', 'adamw').lower()
            learning_rate = getattr(self.config.optimizer, 'lr', 1e-4)
            weight_decay = getattr(self.config.optimizer, 'weight_decay', 1e-4)
        else:
            # ê¸°ë³¸ Config í´ë˜ìŠ¤ ì‚¬ìš©ì‹œ
            optimizer_type = self.config.training.optimizer_type.lower()
            learning_rate = self.config.training.learning_rate
            weight_decay = self.config.training.weight_decay
            
        if optimizer_type == "adam":
            return optim.Adam(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        elif optimizer_type == "adamw":
            return optim.AdamW(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        elif optimizer_type == "sgd":
            return optim.SGD(
                self.model.parameters(),
                lr=learning_rate,
                momentum=0.9,
                weight_decay=weight_decay
            )
        else:
            raise ValueError(f"Unsupported optimizer: {optimizer_type}")
    
    def _create_scheduler(self):
        """Create learning rate scheduler based on configuration."""
        # config êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ‘ê·¼
        if hasattr(self.config, 'scheduler'):
            # massive_dataset_config ì‚¬ìš©ì‹œ
            scheduler_type = getattr(self.config.scheduler, 'type', 'cosine').lower()
            epochs = getattr(self.config.training, 'epochs', 50)
        else:
            # ê¸°ë³¸ Config í´ë˜ìŠ¤ ì‚¬ìš©ì‹œ
            scheduler_type = self.config.training.scheduler_type.lower()
            epochs = self.config.training.epochs
            
        if scheduler_type == "cosine":
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=epochs,
                eta_min=1e-6
            )
        elif scheduler_type == "step":
            step_size = getattr(self.config.training, 'step_size', 15)
            gamma = getattr(self.config.training, 'gamma', 0.1)
            return optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=step_size,
                gamma=gamma
            )
        elif scheduler_type in ["plateau", "reduceonplateau"]:
            factor = getattr(self.config.scheduler, 'factor', 0.5) if hasattr(self.config, 'scheduler') else 0.1
            patience = getattr(self.config.scheduler, 'patience', 5) if hasattr(self.config, 'scheduler') else self.config.training.patience // 2
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                patience=patience,
                factor=factor
            )
        else:
            return None
    
    def train_epoch(self, epoch: int) -> dict:
        """Train one epoch with UberNet-style selective updates."""
        self.model.train()
        total_loss = 0.0
        task_losses = {'detection': 0.0, 'surface': 0.0, 'depth': 0.0}
        task_counts = {'detection': 0, 'surface': 0, 'depth': 0}
        
        pbar = tqdm(self.train_loader, desc=f"ğŸ¯ Epoch {epoch+1}/{self.config.training.epochs}")
        
        for batch_idx, batch in enumerate(pbar):
            # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            images = batch['images'].to(self.device, non_blocking=self.config.system.non_blocking)
            
            # íƒ€ê²Ÿ ì¤€ë¹„
            targets = self._prepare_targets(batch)
            task_mask = targets['task_mask']
            
            # UberNet ë°©ì‹: í•„ìš”í•œ íƒœìŠ¤í¬ë§Œ forward pass
            if self.scaler:
                with torch.amp.autocast('cuda'):
                    outputs = self.model(images, targets, task_mask)  # task_mask ì „ë‹¬
                    loss_dict = self.criterion(outputs, targets, task_mask)
                    loss = loss_dict['total']
            else:
                outputs = self.model(images, targets, task_mask)  # task_mask ì „ë‹¬
                loss_dict = self.criterion(outputs, targets, task_mask)
                loss = loss_dict['total']
            
            # UberNet ë°©ì‹: Selective gradient update
            self.optimizer.zero_grad()
            
            if self.scaler:
                # Scaled backward for mixed precision
                self.scaler.scale(loss).backward()
                
                # UberNet ë°©ì‹: í•´ë‹¹ íƒœìŠ¤í¬ ë¸Œëœì¹˜ + ë°±ë³¸ë§Œ gradient ì—…ë°ì´íŠ¸
                self._selective_gradient_update(task_mask)
                
                self.scaler.unscale_(self.optimizer)
                
                # Gradient clipping (ì—…ë°ì´íŠ¸ë  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ì„œë§Œ)
                max_grad_norm = getattr(self.config.training, 'max_grad_norm', 1.0)
                self._selective_gradient_clipping(task_mask, max_grad_norm)
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                
                # UberNet ë°©ì‹: í•´ë‹¹ íƒœìŠ¤í¬ ë¸Œëœì¹˜ + ë°±ë³¸ë§Œ gradient ì—…ë°ì´íŠ¸
                self._selective_gradient_update(task_mask)
                
                # Gradient clipping (ì—…ë°ì´íŠ¸ë  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ì„œë§Œ)
                max_grad_norm = getattr(self.config.training, 'max_grad_norm', 1.0)
                self._selective_gradient_clipping(task_mask, max_grad_norm)
                
                self.optimizer.step()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            total_loss += loss.item()
            
            # íƒœìŠ¤í¬ë³„ ì†ì‹¤ ì—…ë°ì´íŠ¸ (UberNet ë°©ì‹)
            if task_mask['detection']:
                task_counts['detection'] += 1
                if 'detection_loss' in loss_dict:
                    det_loss = loss_dict['detection_loss']
                    if torch.is_tensor(det_loss):
                        det_loss = det_loss.item()
                    task_losses['detection'] += det_loss
                    
            if task_mask['surface']:
                task_counts['surface'] += 1
                if 'surface_loss' in loss_dict:
                    surf_loss = loss_dict['surface_loss']
                    if torch.is_tensor(surf_loss):
                        surf_loss = surf_loss.item()
                    task_losses['surface'] += surf_loss
                    
            if task_mask['depth']:
                task_counts['depth'] += 1
                if 'depth_loss' in loss_dict:
                    depth_loss = loss_dict['depth_loss']
                    if torch.is_tensor(depth_loss):
                        depth_loss = depth_loss.item()
                    task_losses['depth'] += depth_loss
            
            # ì§„í–‰ë¥  í‘œì‹œ ì—…ë°ì´íŠ¸
            active_tasks = [task for task, active in task_mask.items() if active]
            pbar.set_postfix({
                'Loss': f"{loss.item():.4f}",
                'Tasks': '+'.join(active_tasks),
                'Det': f"{task_losses['detection']/(task_counts['detection']+1e-8):.4f}",
                'Surf': f"{task_losses['surface']/(task_counts['surface']+1e-8):.4f}",
                'Depth': f"{task_losses['depth']/(task_counts['depth']+1e-8):.4f}",
                'LR': f"{self.optimizer.param_groups[0]['lr']:.2e}"
            })
            
            # í…ì„œë³´ë“œ ë¡œê¹…
            if self.writer and batch_idx % self.config.system.print_frequency == 0:
                global_step = epoch * len(self.train_loader) + batch_idx
                self.writer.add_scalar('Train/BatchLoss', loss.item(), global_step)
                self.writer.add_scalar('Train/LearningRate', self.optimizer.param_groups[0]['lr'], global_step)
                
                # íƒœìŠ¤í¬ë³„ ì†ì‹¤ ë¡œê¹…
                for task_name, task_active in task_mask.items():
                    if task_active and f'{task_name}_loss' in loss_dict:
                        self.writer.add_scalar(f'Train/{task_name.capitalize()}Loss', 
                                             loss_dict[f'{task_name}_loss'], global_step)
        
        # ì—í­ í†µê³„ ê³„ì‚°
        avg_loss = total_loss / len(self.train_loader)
        
        return {
            'avg_loss': avg_loss,
            'detection_loss': task_losses['detection'] / (task_counts['detection'] + 1e-8),
            'surface_loss': task_losses['surface'] / (task_counts['surface'] + 1e-8),
            'depth_loss': task_losses['depth'] / (task_counts['depth'] + 1e-8),
            'task_counts': task_counts
        }
    
    def _selective_gradient_update(self, task_mask: Dict[str, bool]):
        """
        Modified UberNet + Cross-task Consistency ë°©ì‹:
        - í•´ë‹¹ íƒœìŠ¤í¬ì˜ ë¸Œëœì¹˜ + ê³µìœ  ë°±ë³¸ ì—…ë°ì´íŠ¸ (UberNet)
        - Cross-task consistencyë¥¼ ìœ„í•´ ê´€ë ¨ íƒœìŠ¤í¬ë“¤ë„ ëª¨ë‘ ì—…ë°ì´íŠ¸
        
        Args:
            task_mask: Which tasks are active in current batch
        """
        # í•­ìƒ ì—…ë°ì´íŠ¸í•  íŒŒë¼ë¯¸í„°: ê³µìœ  ë°±ë³¸
        backbone_params = set(self.model.backbone.parameters())
        
        # íƒœìŠ¤í¬ë³„ íŒŒë¼ë¯¸í„° ìˆ˜ì§‘
        active_task_params = set()
        
        # Detectionì€ ë…ë¦½ì  (ë¼ë²¨ì´ ìˆì„ ë•Œë§Œ)
        if task_mask.get('detection', False):
            active_task_params.update(self.model.detection_head.parameters())
        
        # ğŸŒ‰ Cross-task Consistency: surface ë˜ëŠ” depth ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ë‘˜ ë‹¤ ì—…ë°ì´íŠ¸!
        has_surface_or_depth = (task_mask.get('surface', False) or task_mask.get('depth', False))
        
        if has_surface_or_depth:
            # Surfaceì™€ Depth ëª¨ë‘ ì—…ë°ì´íŠ¸ (consistencyë¥¼ ìœ„í•´)
            active_task_params.update(self.model.surface_head.parameters())
            active_task_params.update(self.model.depth_head.parameters())
            
            # Cross-task projectionsë„ ì—…ë°ì´íŠ¸
            if hasattr(self.model, 'cross_task_heads'):
                active_task_params.update(self.model.cross_task_heads.parameters())
        
        # ì—…ë°ì´íŠ¸í•  íŒŒë¼ë¯¸í„° ì§‘í•©
        params_to_update = backbone_params | active_task_params
        
        # ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ì˜ gradientë¥¼ ì œê±° (Modified UberNet ë°©ì‹)
        for param in self.model.parameters():
            if param not in params_to_update:
                param.grad = None
    
    def _selective_gradient_clipping(self, task_mask: Dict[str, bool], max_grad_norm: float):
        """
        Modified UberNet + Cross-task Consistency ë°©ì‹: ì—…ë°ì´íŠ¸ë  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ì„œë§Œ gradient clipping ì ìš©.
        
        Args:
            task_mask: Which tasks are active in current batch
            max_grad_norm: Maximum gradient norm
        """
        # ì—…ë°ì´íŠ¸í•  íŒŒë¼ë¯¸í„° ìˆ˜ì§‘
        params_to_clip = list(self.model.backbone.parameters())
        
        # Detectionì€ ë…ë¦½ì  (ë¼ë²¨ì´ ìˆì„ ë•Œë§Œ)
        if task_mask.get('detection', False):
            params_to_clip.extend(self.model.detection_head.parameters())
        
        # Cross-task Consistency: surface ë˜ëŠ” depth ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ë‘˜ ë‹¤ í´ë¦¬í•‘
        has_surface_or_depth = (task_mask.get('surface', False) or task_mask.get('depth', False))
        
        if has_surface_or_depth:
            params_to_clip.extend(self.model.surface_head.parameters())
            params_to_clip.extend(self.model.depth_head.parameters())
            
            # Cross-task consistency parameters
            if hasattr(self.model, 'cross_task_heads'):
                params_to_clip.extend(self.model.cross_task_heads.parameters())
        
        # Gradient clipping (ì—…ë°ì´íŠ¸ë  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ì„œë§Œ)
        if params_to_clip:
            torch.nn.utils.clip_grad_norm_(params_to_clip, max_grad_norm)
    
    def _prepare_targets(self, batch: dict) -> dict:
        """Prepare targets for loss computation with UberNet + MTPSL support."""
        targets = {}
        
        # UberNet ë°©ì‹: Task mask ìƒì„± (ë°°ì¹˜ ë ˆë²¨)
        task_mask = {
            'detection': False,
            'surface': False, 
            'depth': False
        }
        
        # ë°°ì¹˜ì—ì„œ ê° íƒœìŠ¤í¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if 'detection_boxes' in batch and len(batch['detection_boxes']) > 0:
            task_mask['detection'] = True
            
        if 'surface_masks' in batch:
            task_mask['surface'] = True
            
        if 'depth_maps' in batch:
            task_mask['depth'] = True
            
        targets['task_mask'] = task_mask
        
        # **Detection targets** - UberNet ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
        if task_mask['detection'] and 'detection_boxes' in batch:
            detection_boxes_list = batch['detection_boxes']
            
            # ê° ë°°ì¹˜ ì•„ì´í…œì˜ boxesë¥¼ deviceë¡œ ì´ë™í•˜ê³  4ì°¨ì› bboxë§Œ ì¶”ì¶œ
            processed_boxes = []
            for boxes in detection_boxes_list:
                if torch.is_tensor(boxes):
                    # 5ì°¨ì›ì—ì„œ 4ì°¨ì› bboxë§Œ ì¶”ì¶œ: [x1, y1, x2, y2, class_id] -> [x1, y1, x2, y2]
                    if boxes.size(-1) >= 4:
                        bbox_only = boxes[:, :4]  # ì²« 4ê°œ ì°¨ì›ë§Œ ì‚¬ìš©
                        processed_boxes.append(bbox_only.to(
                            self.device, non_blocking=self.config.system.non_blocking
                        ))
                    else:
                        # ë¹ˆ í…ì„œ ìƒì„±
                        processed_boxes.append(torch.zeros(0, 4, dtype=torch.float32).to(self.device))
                else:
                    # boxesê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° í…ì„œë¡œ ë³€í™˜í•˜ê³  4ì°¨ì›ë§Œ ì¶”ì¶œ
                    if len(boxes) > 0 and len(boxes[0]) >= 4:
                        bbox_only = [[box[0], box[1], box[2], box[3]] for box in boxes]
                        processed_boxes.append(torch.tensor(
                            bbox_only, dtype=torch.float32
                        ).to(self.device, non_blocking=self.config.system.non_blocking))
                    else:
                        processed_boxes.append(torch.zeros(0, 4, dtype=torch.float32).to(self.device))
            
            targets['detection_boxes'] = processed_boxes
        
        # Detection labelsëŠ” boxesì—ì„œ ì¶”ì¶œ (í´ë˜ìŠ¤ ì •ë³´ê°€ í¬í•¨ë¨)
            detection_labels_list = []
            for boxes in batch['detection_boxes']:
                if torch.is_tensor(boxes) and boxes.size(-1) >= 5:
                    # boxes í˜•íƒœ: [x1, y1, x2, y2, class_id] - 5ì°¨ì› ë°ì´í„°
                    labels = boxes[:, 4].long()  # í´ë˜ìŠ¤ ID ì¶”ì¶œ
                    detection_labels_list.append(labels.to(
                        self.device, non_blocking=self.config.system.non_blocking
                    ))
                else:
                    # ë¹ˆ í…ì„œ ìƒì„±
                    detection_labels_list.append(torch.zeros(0, dtype=torch.long).to(self.device))
            
            targets['detection_labels'] = detection_labels_list
        
        # Surface targets - UberNet ë°©ì‹
        if task_mask['surface'] and 'surface_masks' in batch:
            targets['surface_masks'] = batch['surface_masks'].to(
                self.device, non_blocking=self.config.system.non_blocking
            )
        
        # Depth targets - UberNet ë°©ì‹
        if task_mask['depth'] and 'depth_maps' in batch:
            targets['depth_maps'] = batch['depth_maps'].to(
                self.device, non_blocking=self.config.system.non_blocking
            )
        
        return targets
    
    def validate_epoch(self, epoch: int) -> dict:
        """Validate one epoch with UberNet-style selective computation."""
        self.model.eval()
        total_loss = 0.0
        task_losses = {'detection': 0.0, 'surface': 0.0, 'depth': 0.0}
        task_counts = {'detection': 0, 'surface': 0, 'depth': 0}
        
        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc="ğŸ” Validation")
            
            for batch in pbar:
                images = batch['images'].to(self.device, non_blocking=self.config.system.non_blocking)
                targets = self._prepare_targets(batch)
                task_mask = targets['task_mask']
                
                # UberNet ë°©ì‹: í•„ìš”í•œ íƒœìŠ¤í¬ë§Œ forward pass
                if self.scaler:
                    with torch.amp.autocast('cuda'):
                        outputs = self.model(images, targets, task_mask)  # task_mask ì „ë‹¬
                        loss_dict = self.criterion(outputs, targets, task_mask)
                        loss = loss_dict['total']
                else:
                    outputs = self.model(images, targets, task_mask)  # task_mask ì „ë‹¬
                    loss_dict = self.criterion(outputs, targets, task_mask)
                    loss = loss_dict['total']
                
                total_loss += loss.item()
                
                # íƒœìŠ¤í¬ë³„ ì†ì‹¤ ì—…ë°ì´íŠ¸ (UberNet ë°©ì‹)
                if task_mask['detection'] and 'detection_loss' in loss_dict:
                    task_counts['detection'] += 1
                    det_loss = loss_dict['detection_loss']
                    if torch.is_tensor(det_loss):
                        det_loss = det_loss.item()
                    task_losses['detection'] += det_loss
                    
                if task_mask['surface'] and 'surface_loss' in loss_dict:
                    task_counts['surface'] += 1
                    surf_loss = loss_dict['surface_loss']
                    if torch.is_tensor(surf_loss):
                        surf_loss = surf_loss.item()
                    task_losses['surface'] += surf_loss
                    
                if task_mask['depth'] and 'depth_loss' in loss_dict:
                    task_counts['depth'] += 1
                    depth_loss = loss_dict['depth_loss']
                    if torch.is_tensor(depth_loss):
                        depth_loss = depth_loss.item()
                    task_losses['depth'] += depth_loss
                
                # í™œì„± íƒœìŠ¤í¬ í‘œì‹œ
                active_tasks = [task for task, active in task_mask.items() if active]
                pbar.set_postfix({
                    'Val Loss': f"{loss.item():.4f}",
                    'Tasks': '+'.join(active_tasks)
                })
        
        avg_val_loss = total_loss / len(self.val_loader)
        return {
            'avg_loss': avg_val_loss,
            'detection_loss': task_losses['detection'] / (task_counts['detection'] + 1e-8),
            'surface_loss': task_losses['surface'] / (task_counts['surface'] + 1e-8),
            'depth_loss': task_losses['depth'] / (task_counts['depth'] + 1e-8),
            'task_counts': task_counts
        }
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """Save model checkpoint."""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': self.config.to_dict(),
            'best_val_loss': self.best_val_loss
        }
        
        # Save latest checkpoint
        if self.config.system.save_last:
            latest_path = os.path.join(self.config.data.checkpoint_dir, 'latest_two_task.pth')
            torch.save(checkpoint, latest_path)
        
        # Save best checkpoint
        if is_best and self.config.system.save_best_only:
            best_path = os.path.join(self.config.data.checkpoint_dir, 'best_two_task.pth')
            torch.save(checkpoint, best_path)
            print(f"ğŸ’¾ Best model saved: {best_path}")
    
    def train(self):
        """Main training loop."""
        print(f"\nğŸš€ í›ˆë ¨ ì‹œì‘!")
        print("=" * 70)
        
        start_time = time.time()
        
        for epoch in range(self.config.training.epochs):
            # í›ˆë ¨
            train_metrics = self.train_epoch(epoch)
            
            # ê²€ì¦
            if epoch % self.config.training.val_frequency == 0:
                val_metrics = self.validate_epoch(epoch)
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì²´í¬
                is_best = val_metrics['avg_loss'] < self.best_val_loss
                if is_best:
                    self.best_val_loss = val_metrics['avg_loss']
                    self.patience_counter = 0
                else:
                    self.patience_counter += 1
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if epoch % self.config.training.save_frequency == 0 or is_best:
                    self.save_checkpoint(epoch, is_best)
                
                # í…ì„œë³´ë“œ ë¡œê¹…
                if self.writer:
                    self.writer.add_scalar('Train/EpochLoss', train_metrics['avg_loss'], epoch)
                    self.writer.add_scalar('Val/EpochLoss', val_metrics['avg_loss'], epoch)
                
                # ê²°ê³¼ ì¶œë ¥
                print(f"\nğŸ“Š Epoch {epoch+1} ê²°ê³¼:")
                print(f"   í›ˆë ¨ ì†ì‹¤: {train_metrics['avg_loss']:.4f}")
                print(f"   ê²€ì¦ ì†ì‹¤: {val_metrics['avg_loss']:.4f}")
                print(f"   ìµœê³  ì„±ëŠ¥: {self.best_val_loss:.4f}")
                print(f"   í•™ìŠµë¥ : {self.optimizer.param_groups[0]['lr']:.2e}")
                
                # Early stopping ì²´í¬
                if self.patience_counter >= self.config.training.patience:
                    print(f"â¹ï¸  Early stopping! (patience: {self.config.training.patience})")
                    break
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            if self.scheduler:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_metrics['avg_loss'])
                else:
                    self.scheduler.step()
        
        # í›ˆë ¨ ì™„ë£Œ
        total_time = time.time() - start_time
        print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ì´ ì‹œê°„: {total_time/3600:.2f}ì‹œê°„")
        print(f"   ìµœê³  ê²€ì¦ ì†ì‹¤: {self.best_val_loss:.4f}")
        
        if self.writer:
            self.writer.close()


def main():
    """Main function."""
    print("ğŸ¯ TwoTaskDSPNet í›ˆë ¨ ì‹œì‘")
    print("=" * 70)
    
    # ì„¤ì • ë¡œë“œ - ë¶€ë¶„ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    config = get_subset_training_config()  # âœ… ë¶€ë¶„ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
    # config = get_massive_dataset_config()  # ì „ì²´ ë°ì´í„°ì…‹ ì„¤ì • (ë‚˜ì¤‘ì— ì‚¬ìš©)
    # config = get_full_training_config()  # ì „ì²´ í›ˆë ¨ ì„¤ì •
    # config = get_quick_test_config()  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì„¤ì •
    
    # ì„¤ì • ì¶œë ¥
    config.print_config()
    
    # ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
    print(f"\nğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:")
    print(f"   ê¸°ë³¸ ê²½ë¡œ: {config.data.base_dir}")
    print(f"   ìµœëŒ€ ìƒ˜í”Œ ìˆ˜: {config.dataset.max_samples:,}ê°œ")
    print(f"   ë°°ì¹˜ í¬ê¸°: {config.dataset.batch_size}")
    print(f"   Depth ì‚¬ìš©: {config.dataset.use_depth}")
    
    # ì¬í˜„ì„± ì„¤ì •
    if config.system.seed:
        torch.manual_seed(config.system.seed)
        np.random.seed(config.system.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(config.system.seed)
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨ ì‹œì‘
    trainer = TwoTaskTrainer(config)
    trainer.train()


if __name__ == "__main__":
    main() 