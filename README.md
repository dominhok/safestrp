# safestrp: A Multi-Task Learning Framework

`safestrp`는 이미지 세그멘테이션, 객체 탐지, 깊이 추정을 동시에 수행하는 것을 목표로 하는 PyTorch 기반의 멀티태스크 딥러닝 프레임워크입니다.

## 현재까지 구현된 주요 기능

1.  **모델 아키텍처 정의:**
    *   `resnet_backbone.py`: ResNet-50 아키텍처를 기반으로 하는 백본 네트워크를 구현하여 다양한 다운스트림 태스크에 필요한 특징을 추출합니다.
    *   `dspnet_seg.py`: DSPNet 스타일의 세그멘테이션 헤드를 구현하여 백본 네트워크에서 추출된 특징을 바탕으로 픽셀 단위의 세그멘테이션 마스크를 예측합니다.
    *   `ssd_depth.py`: SSD (Single Shot MultiBox Detector) 스타일의 객체 탐지 헤드와 깊이 추정을 위한 기초적인 구조를 포함합니다. (참고: `ResNetBackbone` 클래스가 `resnet_backbone.py`와 중복 정의되어 있어 추후 리팩토링이 필요합니다.)

2.  **앵커 박스 메커니즘 (객체 탐지용):**
    *   `safestrp/utils/anchors.py`:
        *   `AnchorGenerator`: 다양한 스케일과 종횡비의 SSD 앵커 박스를 생성합니다. 이 설정은 `config.yaml` 파일을 통해 제어됩니다.
        *   `AnchorMatcher`: 생성된 앵커 박스와 실제 Ground Truth 바운딩 박스 간의 매칭 (IoU 기반)을 수행하여 학습 타겟을 준비합니다.
        *   관련 유틸리티 함수 (바운딩 박스 형식 변환, IoU 계산, SSD 방식의 박스 인코딩/디코딩)를 포함합니다.

3.  **멀티태스크 손실 함수:**
    *   `safestrp/losses.py`: 각 태스크에 맞는 다양한 손실 함수를 구현하고, 이들을 조합하여 전체 학습 과정을 관리합니다.
        *   `SegmentationLoss`: 세그멘테이션을 위한 Cross-Entropy 기반 손실 (클래스별 가중치 적용 가능).
        *   `FocalLoss`: 객체 탐지 시 클래스 불균형 문제를 완화하기 위한 Focal Loss.
        *   `ObjectDetectionLoss`: 객체 탐지를 위해 `FocalLoss` (분류용)와 `SmoothL1Loss` (바운딩 박스 회귀용)를 결합한 손실.
        *   `SILogLoss`: 깊이 추정의 정확도를 높이기 위한 Scale-Invariant Logarithmic (SILog) error. (현재 `train.py`에서는 임시로 `SmoothL1Loss`를 사용 중이며, 모델 출력 형식에 맞춰 `SILogLoss` 적용 예정)

4.  **중앙 집중식 설정 관리:**
    *   `safestrp/config.yaml`: 프로젝트의 모든 주요 설정을 한 곳에서 관리합니다.
        *   모델 아키텍처 (백본, 헤드 구성, 클래스 수 등)
        *   학습 파라미터 (배치 크기, 학습률, 에포크 수 등)
        *   데이터셋 경로 및 이미지 전처리 정보
        *   로깅 및 체크포인트 저장 간격
        *   각 태스크별 손실 함수 가중치 및 관련 하이퍼파라미터
        *   앵커 박스 생성 관련 상세 파라미터 (피처맵 스트라이드, 앵커 크기, 종횡비 등)

5.  **학습 파이프라인 (`train.py`):**
    *   `config.yaml`에서 설정을 불러와 전체 학습 과정을 총괄합니다.
    *   데이터 로더 생성 (현재 플레이스홀더, 구현 필요).
    *   모델 (백본, 세그멘테이션 헤드, 탐지/깊이 헤드) 및 손실 함수 초기화.
    *   PyTorch 표준 옵티마이저 및 학습률 스케줄러 설정.
    *   학습 및 검증 루프를 통해 모델을 훈련하고 성능을 평가합니다 (새로운 손실 함수 로직 적용 완료).
    *   학습 과정 및 결과를 TensorBoard를 통해 시각화하고, 주기적으로 모델 체크포인트를 저장합니다.

6.  **프로젝트 의존성 관리:**
    *   `requirements.txt`: 프로젝트 실행에 필요한 Python 라이브러리 및 버전 정보를 명시합니다.

## 미구현 사항 및 향후 개발 계획

*   **데이터 로딩 및 전처리 파이프라인 구축 (`train.py`의 `create_dataloaders` 함수):**
    *   실제 학습에 사용할 데이터셋(예: Cityscapes, KITTI 등)을 로드하고, 이미지 증강(augmentation)을 포함한 전처리 로직을 구현해야 합니다.
    *   이 과정에서 `AnchorMatcher`를 사용하여 객체 탐지 타겟(분류 레이블, 회귀 오프셋)을 생성합니다.
*   **깊이 추정 방식 구체화 및 개선:**
    *   `DSPNet_Detector`가 예측하는 깊이 정보의 형태(예: 각 앵커 박스에 대한 단일 깊이 값, 픽셀 단위의 조밀한 깊이 맵 등)를 명확히 정의해야 합니다.
    *   정의된 깊이 표현 방식에 맞춰 `SILogLoss`를 올바르게 적용하거나, 다른 적절한 깊이 추정 손실 함수를 사용해야 합니다.
*   **예측 결과 후처리:**
    *   객체 탐지: Non-Maximum Suppression (NMS)을 적용하여 중복된 바운딩 박스를 제거하고 최종 예측 결과를 얻습니다.
    *   세그멘테이션 및 깊이 맵: 예측된 결과의 품질을 향상시키기 위한 후처리 기법(예: 필터링, Conditional Random Fields (CRF)) 적용을 고려할 수 있습니다.
*   **통합 테스트 및 평가:**
    *   구현된 모든 모듈(데이터 로더, 모델, 손실 함수, 후처리)을 통합하여 엔드투엔드(end-to-end) 학습 및 추론 파이프라인을 완성합니다.
    *   표준 벤치마크 데이터셋에서 각 태스크(세그멘테이션, 객체 탐지, 깊이 추정)의 성능을 정량적으로 평가합니다.
*   **코드 리팩토링 및 최적화:**
    *   `ssd_depth.py` 내의 중복된 `ResNetBackbone` 정의를 제거하고, `resnet_backbone.py`의 것을 사용하도록 수정합니다.
    *   전체 코드베이스의 가독성, 효율성 및 유지보수성을 향상시키기 위한 리팩토링을 수행합니다.