"""
Cross-task projection heads for MTPSL consistency.

Contains CrossTaskProjectionHead for cross-task consistency learning.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional

from .base import BaseHead


class CrossTaskProjectionHead(BaseHead):
    """
    ðŸ”§ MTPSL ì›ë³¸ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •ëœ Cross-task projection heads.
    
    MTPSL ë…¼ë¬¸ì˜ ì‹¤ì œ êµ¬í˜„ ë°©ì‹ì„ ë”°ë¼:
    1. Task-specific input layersë¡œ ê° íƒœìŠ¤í¬ íŠ¹ì„± ë³´ì¡´
    2. ëª¨ë“  íƒœìŠ¤í¬ë¥¼ ë™ì¼í•œ embedding dimensionìœ¼ë¡œ ë§¤í•‘
    3. ê³µí†µ embedding spaceì—ì„œ ì§ì ‘ cosine similarity ê³„ì‚°
    """
    
    def __init__(self, 
                 seg_channels: int = 7,
                 depth_channels: int = 1,
                 embedding_dim: int = 512,  # MTPSL ì›ë³¸ê³¼ ë™ì¼
                 input_size: Tuple[int, int] = (512, 512),
                 active_task_pairs: Optional[List[Tuple[str, str]]] = None):
        """
        Initialize cross-task projection heads following MTPSL original design.
        
        Args:
            seg_channels: Number of segmentation classes
            depth_channels: Number of depth channels (usually 1)
            embedding_dim: Common embedding dimension (MTPSL ì‚¬ìš© 512)
            input_size: Input image size (H, W)
            active_task_pairs: List of task pairs to enable
        """
        super().__init__()
        
        self.seg_channels = seg_channels
        self.depth_channels = depth_channels
        self.embedding_dim = embedding_dim
        self.input_size = input_size
        
        # Active task pairs (MTPSL ë°©ì‹)
        if active_task_pairs is None:
            self.active_task_pairs = [('surface', 'depth')]
        else:
            self.active_task_pairs = active_task_pairs
        
        print(f"ðŸ”§ MTPSL ì›ë³¸ ë°©ì‹ ì ìš©: {self.active_task_pairs}")
        
        # ðŸ”§ MTPSL ì›ë³¸: Task-specific input channels
        self.task_input_channels = {
            'surface': seg_channels,  # 7 for surface classes
            'depth': depth_channels,  # 1 for depth
        }
        
        # ðŸ”§ MTPSL ì›ë³¸: Task-specific input layers (pre-processing)
        self.task_input_layers = nn.ModuleDict()
        for task_name, channels in self.task_input_channels.items():
            self.task_input_layers[f'{task_name}_input'] = nn.Sequential(
                nn.Conv2d(channels, 64, kernel_size=3, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True)
            )
        
        # ðŸ”§ MTPSL ì›ë³¸: Shared mapping function (SegNet encoder ìŠ¤íƒ€ì¼)
        # ëª¨ë“  íƒœìŠ¤í¬ê°€ ë™ì¼í•œ embeddingìœ¼ë¡œ ë§¤í•‘ë¨
        filter_sizes = [64, 128, 256, 512]
        self.shared_encoder = nn.ModuleList()
        
        in_channels = 64  # task-specific input layer ì¶œë ¥
        for out_channels in filter_sizes:
            self.shared_encoder.append(
                nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(kernel_size=2, stride=2)
                )
            )
            in_channels = out_channels
        
        # ðŸ”§ Final embedding layer: 512ì°¨ì› ê³ ì • (MTPSL ì›ë³¸)
        self.final_embedding = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(512, embedding_dim),
            nn.ReLU(inplace=True)
        )
        
        self._init_weights()
        
        print(f"âœ… MTPSL ì›ë³¸ ë°©ì‹ CrossTaskProjectionHead:")
        print(f"   ìž„ë² ë”© ì°¨ì›: {embedding_dim} (ëª¨ë“  íƒœìŠ¤í¬ ê³µí†µ)")
        print(f"   íƒœìŠ¤í¬ë³„ ìž…ë ¥ ì±„ë„: {self.task_input_channels}")
        print(f"   í™œì„± íƒœìŠ¤í¬ ìŒ: {len(self.active_task_pairs)}ê°œ")

    def forward(self, 
                seg_gt: Optional[torch.Tensor] = None,
                seg_pred: Optional[torch.Tensor] = None,
                depth_gt: Optional[torch.Tensor] = None,
                depth_pred: Optional[torch.Tensor] = None,
                task_mask: Optional[Dict[str, bool]] = None) -> Dict[str, torch.Tensor]:
        """
        ðŸ”§ MTPSL ì›ë³¸ ë°©ì‹ì˜ forward pass.
        
        ëª¨ë“  íƒœìŠ¤í¬ë¥¼ ë™ì¼í•œ embedding dimensionìœ¼ë¡œ ë§¤í•‘í•˜ì—¬
        ì§ì ‘ cosine similarity ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë„ë¡ í•¨.
        """
        embeddings = {}
        
        if task_mask is None:
            task_mask = {'surface': True, 'depth': True}
        
        # Task data mapping
        task_data = {
            'surface': {'gt': seg_gt, 'pred': seg_pred},
            'depth': {'gt': depth_gt, 'pred': depth_pred}
        }
        
        # ðŸ”§ MTPSL ì›ë³¸: ê° íƒœìŠ¤í¬ë³„ë¡œ ë™ì¼í•œ pipeline ì ìš©
        for task_name, data in task_data.items():
            if not task_mask.get(task_name, False):
                continue
                
            gt_data = data['gt']
            pred_data = data['pred']
            
            # GT processing
            if gt_data is not None:
                # ðŸ”§ Pre-process GT (MTPSL ì›ë³¸ ë°©ì‹)
                gt_processed = self._preprocess_gt(gt_data, task_name)
                gt_embedding = self._extract_embedding(gt_processed, task_name)
                embeddings[f'{task_name}_gt_embedding'] = gt_embedding
            
            # Prediction processing  
            if pred_data is not None:
                # ðŸ”§ Pre-process Prediction (MTPSL ì›ë³¸ ë°©ì‹)
                pred_processed = self._preprocess_pred(pred_data, task_name)
                pred_embedding = self._extract_embedding(pred_processed, task_name)
                embeddings[f'{task_name}_pred_embedding'] = pred_embedding
        
        return embeddings

    def _preprocess_gt(self, gt: torch.Tensor, task: str) -> torch.Tensor:
        """
        ðŸ”§ MTPSL ì›ë³¸ì˜ GT preprocessing.
        """
        if task == 'surface':
            if gt.dim() == 3:  # (B, H, W) class indices
                # Convert to one-hot: (B, H, W) -> (B, C, H, W)
                gt = F.one_hot(gt.long(), self.seg_channels).permute(0, 3, 1, 2).float()
            # MTPSL ì›ë³¸: ignore -1 labels ì²˜ë¦¬ëŠ” ìƒëžµ (SafeStrpì—ì„œëŠ” ë¶ˆí•„ìš”)
            return gt
        elif task == 'depth':
            # MTPSL ì›ë³¸: normalize by max
            gt_normalized = gt / (gt.max() + 1e-12)
            return gt_normalized
        return gt

    def _preprocess_pred(self, pred: torch.Tensor, task: str) -> torch.Tensor:
        """
        ðŸ”§ MTPSL ì›ë³¸ì˜ Prediction preprocessing.
        """
        if task == 'surface':
            # MTPSL ì›ë³¸: Gumbel-Softmax for hard assignment
            pred_processed = F.gumbel_softmax(pred, dim=1, tau=1, hard=True)
            # Handle potential NaN (MTPSL ì›ë³¸ ë°©ì‹)
            while torch.isnan(pred_processed.sum()):
                pred_processed = F.gumbel_softmax(pred, dim=1, tau=1, hard=True)
            return pred_processed
        elif task == 'depth':
            # MTPSL ì›ë³¸: normalize by max
            pred_normalized = pred / (pred.max() + 1e-12)
            return pred_normalized
        return pred

    def _extract_embedding(self, x: torch.Tensor, task: str) -> torch.Tensor:
        """
        ðŸ”§ MTPSL ì›ë³¸: ëª¨ë“  íƒœìŠ¤í¬ë¥¼ ë™ì¼í•œ embeddingìœ¼ë¡œ ë³€í™˜.
        """
        # Task-specific input layer
        x = self.task_input_layers[f'{task}_input'](x)
        
        # Shared encoder (ëª¨ë“  íƒœìŠ¤í¬ ê³µí†µ)
        for encoder_block in self.shared_encoder:
            x = encoder_block(x)
        
        # Final embedding (512ì°¨ì› ê³ ì •)
        embedding = self.final_embedding(x)
        
        return embedding

    def compute_cross_task_loss(self, embeddings: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        ðŸ”§ MTPSL ì›ë³¸ ë°©ì‹ì˜ cross-task consistency loss.
        
        ëª¨ë“  embeddingì´ ë™ì¼í•œ ì°¨ì›(512)ì´ë¯€ë¡œ ì§ì ‘ cosine similarity ê³„ì‚° ê°€ëŠ¥.
        """
        total_loss = 0.0
        loss_count = 0
        
        for task1, task2 in self.active_task_pairs:
            # Cross-task consistency: task1_pred â†” task2_gt
            task1_pred_key = f'{task1}_pred_embedding'
            task2_gt_key = f'{task2}_gt_embedding'
            
            if task1_pred_key in embeddings and task2_gt_key in embeddings:
                task1_pred_emb = embeddings[task1_pred_key]  # (B, 512)
                task2_gt_emb = embeddings[task2_gt_key]      # (B, 512)
                
                # ðŸ”§ MTPSL ì›ë³¸: 1 - cosine_similarity
                cosine_sim = F.cosine_similarity(task1_pred_emb, task2_gt_emb, dim=1, eps=1e-12)
                loss = 1.0 - cosine_sim.mean()
                total_loss += loss
                loss_count += 1
            
            # Reverse direction: task2_pred â†” task1_gt
            task2_pred_key = f'{task2}_pred_embedding'
            task1_gt_key = f'{task1}_gt_embedding'
            
            if task2_pred_key in embeddings and task1_gt_key in embeddings:
                task2_pred_emb = embeddings[task2_pred_key]  # (B, 512)
                task1_gt_emb = embeddings[task1_gt_key]      # (B, 512)
                
                cosine_sim = F.cosine_similarity(task2_pred_emb, task1_gt_emb, dim=1, eps=1e-12)
                loss = 1.0 - cosine_sim.mean()
                total_loss += loss
                loss_count += 1
        
        if loss_count > 0:
            return total_loss / loss_count
        else:
            return torch.tensor(0.0, device=next(iter(embeddings.values())).device)

    def get_embedding_info(self) -> Dict[str, int]:
        """ìž„ë² ë”© ì •ë³´ ë°˜í™˜."""
        return {
            'embedding_dim': self.embedding_dim,
            'task_input_channels': self.task_input_channels,
            'active_pairs': len(self.active_task_pairs)
        }

    def get_active_pairs(self) -> List[Tuple[str, str]]:
        """í˜„ìž¬ í™œì„±í™”ëœ task pair ëª©ë¡ ë°˜í™˜."""
        return self.active_task_pairs.copy()

    def compute_gt_embeddings(self, targets: Dict) -> Dict[str, torch.Tensor]:
        """
        ðŸ”§ GT ë°ì´í„°ë¡œë¶€í„° embedding ê³„ì‚° (MTPSL ì›ë³¸ ë°©ì‹).
        
        Args:
            targets: íƒ€ê²Ÿ ë°ì´í„° (surface_masks, depth_maps ë“±)
            
        Returns:
            GT embeddings dictionary
        """
        gt_embeddings = {}
        
        # Surface GT embedding
        if 'surface_masks' in targets:
            surface_gt = targets['surface_masks']  # (B, H, W) - class indices
            
            # One-hot encodingìœ¼ë¡œ ë³€í™˜ (B, H, W) -> (B, C, H, W)
            batch_size, height, width = surface_gt.shape
            surface_gt_onehot = torch.zeros(
                batch_size, self.task_input_channels['surface'], height, width,
                device=surface_gt.device
            )
            surface_gt_onehot.scatter_(1, surface_gt.unsqueeze(1), 1.0)
            
            # Task-specific processing
            surface_processed = self.task_input_layers['surface_input'](surface_gt_onehot)
            
            # Shared encoder
            for encoder_block in self.shared_encoder:
                surface_processed = encoder_block(surface_processed)
            
            # Final embedding
            surface_gt_emb = self.final_embedding(surface_processed)
            gt_embeddings['surface_gt_embedding'] = surface_gt_emb
        
        # Depth GT embedding
        if 'depth_maps' in targets:
            depth_gt = targets['depth_maps']  # (B, 1, H, W)
            
            # Task-specific processing
            depth_processed = self.task_input_layers['depth_input'](depth_gt)
            
            # Shared encoder
            for encoder_block in self.shared_encoder:
                depth_processed = encoder_block(depth_processed)
            
            # Final embedding
            depth_gt_emb = self.final_embedding(depth_processed)
            gt_embeddings['depth_gt_embedding'] = depth_gt_emb
        
        return gt_embeddings


def cosine_similarity_loss(embedding1: torch.Tensor, embedding2: torch.Tensor) -> torch.Tensor:
    """
    Compute cosine similarity loss between two embeddings (MTPSL style).
    
    Args:
        embedding1: (B, C)
        embedding2: (B, C)
        
    Returns:
        Cosine similarity loss (scalar)
    """
    # Compute cosine similarity
    cosine_sim = F.cosine_similarity(embedding1, embedding2, dim=1, eps=1e-12)
    
    # Convert to loss (1 - cosine_similarity)
    cosine_loss = 1.0 - cosine_sim.mean()
    
    return cosine_loss 